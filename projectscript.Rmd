---
title: "Machine Learning Excercise."
author: "Aaron Nematnejad"
date: "Sunday, June 21, 2015"
output: html_document
---

subset the data by getting rid of appropriate columns. 
Attach final file to the github repository. 


```{r}
library(caret)
setwd("C:/Users/aaron/Desktop/Coursera/Machine")
nike=read.csv("pml-training.csv")


#nike=nike[1:4000,]
#This just read in the training spreadsheet 

str(nike)
dim(nike)
table(nike$classe)

#What needs to be done now slice the data using 70% for the trauining set and 30% for the test set. 

nike=createDataPartition(nike$classe, p=0.70, list=FALSE)

#After performingh the preprocessings and testing using principal component analysis, I decided that 3 columns had the #most significance. Other columns helped with the accuracy but at a much greater computational cost. 

#choosing only 3 columns 
nike=nike[,c("magnet_dumbbell_z","yaw_forearm","magnet_forearm_z","classe")]

```


```{r}

#To demonstrate output lets start with rpart


 

modfit=train(classe~.,method="rpart", data=nike)
modfit

#Lets see what ahppens if I do modfit$finalModel
modfit$finalModel
plot(modfit$final,uniform=TRUE,main="Classification Tree")
text(modfit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)
#fancier plots 
library(rattle)
fancyRpartPlot(modfit$finalModel)

```

